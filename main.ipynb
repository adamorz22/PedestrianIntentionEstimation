{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79kUt9OkztCd"
   },
   "source": [
    "# Generate Bounding Boxes and Pose Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGo5LWt1zyU2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOOLqtS_CRfn"
   },
   "outputs": [],
   "source": [
    "# Path to JAAD dataset and output directory\n",
    "jaad_path = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\JAAD_clips'\n",
    "annotation_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\annotations'\n",
    "output_dir = os.path.join(jaad_path, 'frames_with_bboxes')\n",
    "pose_output_dir = os.path.join(jaad_path, 'pose_keypoints')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(pose_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-9Ul6CiCRma",
    "outputId": "9b0cd29f-33f2-4bc4-941f-5bbc7a38dc27"
   },
   "outputs": [],
   "source": [
    "# Initialize Mediapipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=False, min_detection_confidence=0.4)\n",
    "\n",
    "def draw_keypoints(image, keypoints):\n",
    "    for keypoint in keypoints:\n",
    "        x = int(keypoint[0] * image.shape[1])\n",
    "        y = int(keypoint[1] * image.shape[0])\n",
    "        cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_and_save_frames_with_bboxes_and_pose_keypoints(\n",
    "        video_path, annotation_path, video_output_dir, pose_output_dir,\n",
    "        pad=0.15, min_box=40, min_det_conf=0.7, min_vis=0.5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    os.makedirs(video_output_dir, exist_ok=True)\n",
    "    os.makedirs(pose_output_dir, exist_ok=True)\n",
    "\n",
    "    tree = ET.parse(annotation_path); root = tree.getroot()\n",
    "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    eps = 1e-6\n",
    "\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1,\n",
    "                        enable_segmentation=False,\n",
    "                        min_detection_confidence=min_det_conf,\n",
    "                        min_tracking_confidence=min_det_conf)\n",
    "\n",
    "    def boxes_for_frame(fid):\n",
    "        out = []\n",
    "        for track in root.findall('.//track'):\n",
    "            for box in track.findall('.//box'):\n",
    "                if int(box.get('frame')) == fid:\n",
    "                    xtl = int(float(box.get('xtl'))); ytl = int(float(box.get('ytl')))\n",
    "                    xbr = int(float(box.get('xbr'))); ybr = int(float(box.get('ybr')))\n",
    "                    out.append((xtl, ytl, xbr, ybr))\n",
    "        return out\n",
    "\n",
    "    frame_id = 0\n",
    "    with tqdm(total=total_frames, desc=f\"Processing {video_name}\", unit=\"frame\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame_full = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            boxes = boxes_for_frame(frame_id)\n",
    "            for (xtl, ytl, xbr, ybr) in boxes:\n",
    "                cv2.rectangle(frame_full, (xtl, ytl), (xbr, ybr), (0, 255, 0), 2)\n",
    "\n",
    "            keypoints = np.zeros((33, 3), dtype=np.float32)\n",
    "\n",
    "            if boxes:\n",
    "                # choose biggest box\n",
    "                areas = [ (xbr-xtl)*(ybr-ytl) for (xtl, ytl, xbr, ybr) in boxes ]\n",
    "                xtl, ytl, xbr, ybr = boxes[int(np.argmax(areas))]\n",
    "                bw, bh = xbr-xtl, ybr-ytl\n",
    "\n",
    "                if bw >= min_box and bh >= min_box:\n",
    "                    px = int(pad * bw); py = int(pad * bh)\n",
    "                    cx1 = max(0, xtl - px); cy1 = max(0, ytl - py)\n",
    "                    cx2 = min(W, xbr + px); cy2 = min(H, ybr + py)\n",
    "\n",
    "                    if cx2 > cx1 and cy2 > cy1:\n",
    "                        crop = frame_full[cy1:cy2, cx1:cx2]\n",
    "                        r = pose.process(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                        if r.pose_landmarks:\n",
    "                            lm = r.pose_landmarks.landmark\n",
    "                            cw = max(1, cx2 - cx1); ch = max(1, cy2 - cy1)\n",
    "                            out_pts = []\n",
    "                            for p in lm:\n",
    "                                vis = float(p.visibility) if (p.visibility is not None and np.isfinite(p.visibility)) else 0.0\n",
    "                                zval = float(p.z) if (p.z is not None and np.isfinite(p.z)) else 0.0\n",
    "\n",
    "                                if vis < min_vis:\n",
    "                                    out_pts.append([0.0, 0.0, 0.0])\n",
    "                                    continue\n",
    "\n",
    "                                # remap to full frame pixels\n",
    "                                x_pix = int(max(0, min(cw-1, p.x * cw))) + cx1\n",
    "                                y_pix = int(max(0, min(ch-1, p.y * ch))) + cy1\n",
    "\n",
    "                                x_pix = int(max(0, min(W-1, x_pix)))\n",
    "                                y_pix = int(max(0, min(H-1, y_pix)))\n",
    "\n",
    "                                x_n = x_pix / (W - 1 + eps)\n",
    "                                y_n = y_pix / (H - 1 + eps)\n",
    "                                out_pts.append([x_n, y_n, zval])\n",
    "\n",
    "                                cv2.circle(frame_full, (x_pix, y_pix), 4, (0, 0, 255), -1)\n",
    "\n",
    "                            keypoints = np.array(out_pts, dtype=np.float32)\n",
    "\n",
    "            keypoints = np.nan_to_num(keypoints, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "\n",
    "            preview = cv2.resize(frame_full, (640, 480), interpolation=cv2.INTER_AREA)\n",
    "            frame_filename = os.path.join(video_output_dir, f\"{video_name}_frame_{frame_id:05d}.jpg\")\n",
    "            keypoints_filename = os.path.join(pose_output_dir,  f\"{video_name}_frame_{frame_id:05d}.npy\")\n",
    "            cv2.imwrite(frame_filename, preview)\n",
    "            np.save(keypoints_filename, keypoints)\n",
    "\n",
    "            frame_id += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "#One-batch-per-run processing\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "def is_already_processed(video_file: str) -> bool:\n",
    "    vid_name = os.path.splitext(video_file)[0]\n",
    "    frames_dir_vid = os.path.join(output_dir, vid_name)\n",
    "    kpts_dir_vid   = os.path.join(pose_output_dir, vid_name)\n",
    "\n",
    "    frames_ok = os.path.isdir(frames_dir_vid) and any(\n",
    "        f.lower().endswith('.jpg') for f in os.listdir(frames_dir_vid)\n",
    "    )\n",
    "    kpts_ok = os.path.isdir(kpts_dir_vid) and any(\n",
    "        f.lower().endswith('.npy') for f in os.listdir(kpts_dir_vid)\n",
    "    )\n",
    "    return frames_ok or kpts_ok\n",
    "\n",
    "all_videos = sorted([f for f in os.listdir(jaad_path) if f.lower().endswith('.mp4')])\n",
    "all_videos = [vf for vf in all_videos\n",
    "              if os.path.exists(os.path.join(annotation_dir, vf.replace('.mp4', '.xml')))]\n",
    "\n",
    "# choose unprocessed\n",
    "remaining = [vf for vf in all_videos if not is_already_processed(vf)]\n",
    "\n",
    "print(f\"[INFO] Total videos: {len(all_videos)} | Already processed: {len(all_videos) - len(remaining)} | Remaining: {len(remaining)}\")\n",
    "\n",
    "if not remaining:\n",
    "    print(\"[INFO] Nothing to do. All videos are already processed.\")\n",
    "else:\n",
    "    current_batch = remaining[:BATCH_SIZE]\n",
    "    print(f\"[INFO] Processing one batch: {len(current_batch)} videos\")\n",
    "\n",
    "    for video_file in current_batch:\n",
    "        video_path = os.path.join(jaad_path, video_file)\n",
    "        annotation_file = video_file.replace('.mp4', '.xml')\n",
    "        annotation_path = os.path.join(annotation_dir, annotation_file)\n",
    "\n",
    "        video_output_dir = os.path.join(output_dir, os.path.splitext(video_file)[0])\n",
    "        video_pose_output_dir = os.path.join(pose_output_dir, os.path.splitext(video_file)[0])\n",
    "\n",
    "        os.makedirs(video_output_dir, exist_ok=True)\n",
    "        os.makedirs(video_pose_output_dir, exist_ok=True)\n",
    "\n",
    "        extract_and_save_frames_with_bboxes_and_pose_keypoints(\n",
    "            video_path, annotation_path, video_output_dir, video_pose_output_dir\n",
    "        )\n",
    "\n",
    "    left_after = len(remaining) - len(current_batch)\n",
    "    print(f\"\\n[INFO] Batch done. Remaining videos to process next runs: {left_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_generated_files(frames_root, kpts_root):\n",
    "    def count_ext(root, ext):\n",
    "        total = 0\n",
    "        per_video = {}\n",
    "        if not os.path.isdir(root):\n",
    "            return 0, {}\n",
    "        for vd in sorted(os.listdir(root)):\n",
    "            vpath = os.path.join(root, vd)\n",
    "            if not os.path.isdir(vpath):\n",
    "                continue\n",
    "            n = sum(1 for f in os.listdir(vpath) if f.lower().endswith(ext))\n",
    "            per_video[vd] = n\n",
    "            total += n\n",
    "        return total, per_video\n",
    "\n",
    "    total_jpg, jpg_per_video = count_ext(frames_root, \".jpg\")\n",
    "    total_npy, npy_per_video = count_ext(kpts_root, \".npy\")\n",
    "\n",
    "    all_vids = sorted(set(list(jpg_per_video.keys()) + list(npy_per_video.keys())))\n",
    "    mismatches = []\n",
    "    missing_frames = []\n",
    "    missing_kpts = []\n",
    "\n",
    "    for vid in all_vids:\n",
    "        n_jpg = jpg_per_video.get(vid, 0)\n",
    "        n_npy = npy_per_video.get(vid, 0)\n",
    "        if n_jpg == 0 and n_npy > 0:\n",
    "            missing_frames.append((vid, n_jpg, n_npy))\n",
    "        elif n_npy == 0 and n_jpg > 0:\n",
    "            missing_kpts.append((vid, n_jpg, n_npy))\n",
    "        elif n_jpg != n_npy:\n",
    "            mismatches.append((vid, n_jpg, n_npy))\n",
    "\n",
    "    return total_jpg, total_npy, mismatches, missing_frames, missing_kpts\n",
    "\n",
    "total_jpg, total_npy, mismatches, missing_frames, missing_kpts = count_generated_files(\n",
    "    output_dir, pose_output_dir\n",
    ")\n",
    "\n",
    "print(\"\\n PODSUMOWANIE WYGENEROWANYCH PLIKÓW \")\n",
    "print(f\"Łącznie klatek (JPG):      {total_jpg}\")\n",
    "print(f\"Łącznie keypointów (NPY):  {total_npy}\")\n",
    "print(f\"Wideo z brakującymi KLATKAMI (jpg, ale są npy): {len(missing_frames)}\")\n",
    "print(f\"Wideo z brakującymi KEYPOINTAMI (npy, ale są jpg): {len(missing_kpts)}\")\n",
    "print(f\"Wideo z NIESPÓJNYMI LICZBAMI jpg != npy: {len(mismatches)}\")\n",
    "\n",
    "def preview(rows, title, k=5):\n",
    "    if not rows:\n",
    "        return\n",
    "    print(f\"\\n{title} (pokazuję do {k}):\")\n",
    "    for vid, n_jpg, n_npy in rows[:k]:\n",
    "        print(f\"  {vid}: jpg={n_jpg}, npy={n_npy}\")\n",
    "\n",
    "preview(missing_frames, \"Brakuje KLATEK (jpg, a są npy)\")\n",
    "preview(missing_kpts,   \"Brakuje KEYPOINTÓW (npy, a są jpg)\")\n",
    "preview(mismatches,     \"Niespójne liczby plików (jpg != npy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRFR5XuVCsq-"
   },
   "source": [
    "# Preprocess Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46pHA4Q8CzUb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRujbm7LC43O"
   },
   "outputs": [],
   "source": [
    "def preprocess_annotations(annotations_base_dir, cache_dir, video_names):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    total_crossing = 0\n",
    "    total_not_crossing = 0\n",
    "\n",
    "\n",
    "    annotations_dir = os.path.join(annotations_base_dir, 'annotations')\n",
    "    annotations_dirs = {\n",
    "        'attributes': os.path.join(annotations_base_dir, 'annotations_attributes'),\n",
    "        'appearance': os.path.join(annotations_base_dir, 'annotations_appearance'),\n",
    "        'traffic': os.path.join(annotations_base_dir, 'annotations_traffic'),\n",
    "        'vehicle': os.path.join(annotations_base_dir, 'annotations_vehicle')\n",
    "    }\n",
    "\n",
    "    for video in video_names:\n",
    "        video_id = video.split('_')[1].split('.')[0]\n",
    "\n",
    "        annotations_paths = {\n",
    "            'annotations': os.path.join(annotations_dir, f\"video_{video_id}.xml\"),\n",
    "            'attributes': os.path.join(annotations_dirs['attributes'], f\"video_{video_id}_attributes.xml\"),\n",
    "            'appearance': os.path.join(annotations_dirs['appearance'], f\"video_{video_id}_appearance.xml\"),\n",
    "            'traffic': os.path.join(annotations_dirs['traffic'], f\"video_{video_id}_traffic.xml\"),\n",
    "            'vehicle': os.path.join(annotations_dirs['vehicle'], f\"video_{video_id}_vehicle.xml\")\n",
    "        }\n",
    "\n",
    "        annotations = {key: ET.parse(path).getroot() for key, path in annotations_paths.items() if os.path.exists(path)}\n",
    "\n",
    "        preprocessed_data = []\n",
    "\n",
    "        for track in annotations['annotations'].findall('.//track'):\n",
    "            for box in track.findall('.//box'):\n",
    "                frame_id = int(box.get('frame'))\n",
    "                \n",
    "                # find biggest pedestrian in frame\n",
    "                boxes_in_frame = []\n",
    "                for tr in annotations['annotations'].findall('.//track'):\n",
    "                    for bx in tr.findall('.//box'):\n",
    "                        if int(bx.get('frame')) == frame_id:\n",
    "                            xtl = float(bx.get('xtl'))\n",
    "                            ytl = float(bx.get('ytl'))\n",
    "                            xbr = float(bx.get('xbr'))\n",
    "                            ybr = float(bx.get('ybr'))\n",
    "                            area = (xbr - xtl) * (ybr - ytl)\n",
    "                            boxes_in_frame.append((bx, area))\n",
    "                if not boxes_in_frame:\n",
    "                    continue\n",
    "                # choose biggest box\n",
    "                largest_box, _ = max(boxes_in_frame, key=lambda x: x[1])\n",
    "\n",
    "                if box != largest_box:\n",
    "                    continue\n",
    "\n",
    "                # label crossing / not crossing\n",
    "                label = 0\n",
    "                for attr in box.findall('attribute'):\n",
    "                    if attr.get('name') == 'cross' and attr.text == 'crossing':\n",
    "                        label = 1\n",
    "                        break\n",
    "\n",
    "                if label == 1:\n",
    "                    total_crossing += 1\n",
    "                else:\n",
    "                    total_not_crossing += 1\n",
    "\n",
    "                # additional info\n",
    "                traffic_info = get_traffic_info(annotations.get('traffic', None), frame_id)\n",
    "                vehicle_info = get_vehicle_info(annotations.get('vehicle', None), frame_id)\n",
    "                appearance_info = get_appearance_info(annotations.get('appearance', None), frame_id)\n",
    "                attributes_info = get_attributes_info(annotations.get('attributes', None), frame_id)\n",
    "\n",
    "                preprocessed_data.append((frame_id, label, traffic_info, vehicle_info, appearance_info, attributes_info))\n",
    "\n",
    "\n",
    "        with open(os.path.join(cache_dir, f\"video_{video_id}.pkl\"), 'wb') as f:\n",
    "            pickle.dump(preprocessed_data, f)\n",
    "\n",
    "    print(f\"\\nTotal 'crossing' samples: {total_crossing}\")\n",
    "    print(f\"Total 'not crossing' samples: {total_not_crossing}\")\n",
    "\n",
    "def get_traffic_info(root, frame_id):\n",
    "    traffic_info = {'ped_crossing': 0, 'ped_sign': 0, 'stop_sign': 0, 'traffic_light': 0}\n",
    "    if root is not None:\n",
    "        for frame in root.findall('.//frame'):\n",
    "            if int(frame.get('id')) == frame_id:\n",
    "                traffic_info = {\n",
    "                    'ped_crossing': int(frame.get('ped_crossing')),\n",
    "                    'ped_sign': int(frame.get('ped_sign')),\n",
    "                    'stop_sign': int(frame.get('stop_sign')),\n",
    "                    'traffic_light': 1 if frame.get('traffic_light') != 'n/a' else 0\n",
    "                }\n",
    "    return traffic_info\n",
    "\n",
    "def get_vehicle_info(root, frame_id):\n",
    "    vehicle_info = {'action': 0}\n",
    "    if root is not None:\n",
    "        for frame in root.findall('.//frame'):\n",
    "            if int(frame.get('id')) == frame_id:\n",
    "                action = frame.get('action')\n",
    "                vehicle_info = {\n",
    "                    'action': 1 if action == 'moving_slow' else 2 if action == 'decelerating' else 3 if action == 'stopped' else 4 if action is not None and action == 'accelerating' else 0\n",
    "                }\n",
    "    return vehicle_info\n",
    "\n",
    "def get_appearance_info(root, frame_id):\n",
    "    appearance_info = {'pose': 0, 'clothing': 0, 'objects': 0}\n",
    "    if root is not None:\n",
    "        for track in root.findall('.//track'):\n",
    "            for box in track.findall('.//box'):\n",
    "                if int(box.get('frame')) == frame_id:\n",
    "                    appearance_info = {\n",
    "                        'pose': float(box.find('pose').text) if box.find('pose') is not None else 0,\n",
    "                        'clothing': float(box.find('clothing').text) if box.find('clothing') is not None else 0,\n",
    "                        'objects': float(box.find('objects').text) if box.find('objects') is not None else 0\n",
    "                    }\n",
    "    return appearance_info\n",
    "\n",
    "def get_attributes_info(root, frame_id):\n",
    "    attributes_info = {'age': 0, 'gender': 0, 'crossing_point': 0}\n",
    "    if root is not None:\n",
    "        for track in root.findall('.//track'):\n",
    "            for box in track.findall('.//box'):\n",
    "                if int(box.get('frame')) == frame_id:\n",
    "                    attributes_info = {\n",
    "                        'age': float(box.find('age').text) if box.find('age') is not None else 0,\n",
    "                        'gender': 1 if box.find('gender') is not None and box.find('gender').text == 'male' else 0 if box.find('gender') is not None and box.find('gender').text == 'female' else 0,\n",
    "                        'crossing_point': float(box.find('crossing_point').text) if box.find('crossing_point') is not None else 0\n",
    "                    }\n",
    "    return attributes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHsXUYG1D7RO",
    "outputId": "988ccfd7-3e27-4149-b06d-5fec4ca7c53c"
   },
   "outputs": [],
   "source": [
    "annotations_base_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset'\n",
    "cache_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\cache'\n",
    "video_names = sorted(os.listdir(os.path.join(annotations_base_dir, 'annotations')))\n",
    "preprocess_annotations(annotations_base_dir, cache_dir, video_names)\n",
    "\n",
    "print(\"Annotations preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHUDok15DBc2"
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAeGTUCZDDyu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from multiprocessing import Value, cpu_count\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2kHl7yYDMHK",
    "outputId": "0d145182-33c9-4c7a-ba21-9871a951efc2"
   },
   "outputs": [],
   "source": [
    "counter = Value('i', 0)\n",
    "\n",
    "class JAADDataset(Dataset):\n",
    "    def __init__(self, frames_dir, keypoints_dir, cache_dir, transform=None):\n",
    "        self.frames_dir = frames_dir\n",
    "        self.keypoints_dir = keypoints_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.transform = transform\n",
    "        self.video_names = sorted(os.listdir(frames_dir))\n",
    "        self.data = []\n",
    "\n",
    "        for video in self.video_names:\n",
    "            video_id = video.split('_')[1]\n",
    "            cache_file = os.path.join(cache_dir, f\"video_{video_id}.pkl\")\n",
    "            if not os.path.exists(cache_file):\n",
    "                continue\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                video_data = pickle.load(f)\n",
    "                for frame_id, label, traffic_info, vehicle_info, appearance_info, attributes_info in video_data:\n",
    "                    frame_path = os.path.join(frames_dir, video, f\"{video}_frame_{frame_id:05d}.jpg\")\n",
    "                    keypoint_file = os.path.join(keypoints_dir, video, f\"{video}_frame_{frame_id:05d}.npy\")\n",
    "                    if os.path.exists(frame_path) and os.path.exists(keypoint_file):\n",
    "                        self.data.append((frame_path, keypoint_file, label, traffic_info, vehicle_info, appearance_info, attributes_info))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with counter.get_lock():\n",
    "            counter.value += 1\n",
    "            if counter.value % 1000 == 0:\n",
    "                print(f\"[INFO] __getitem__ called {counter.value} times\", flush=True)\n",
    "\n",
    "        frame_path, keypoint_file, label, traffic_info, vehicle_info, appearance_info, attributes_info = self.data[idx]\n",
    "        frame = Image.open(frame_path).convert('RGB')\n",
    "\n",
    "        keypoints = np.load(keypoint_file)\n",
    "        if keypoints.size == 0:  # Handle empty keypoints\n",
    "            keypoints = np.zeros((33, 3), dtype=np.float32)\n",
    "\n",
    "        keypoints = torch.tensor(keypoints, dtype=torch.float32)\n",
    "        traffic_info = torch.tensor(list(traffic_info.values()), dtype=torch.float32)\n",
    "        vehicle_info = torch.tensor(list(vehicle_info.values()), dtype=torch.float32)\n",
    "        appearance_info = torch.tensor(list(appearance_info.values()), dtype=torch.float32)\n",
    "        attributes_info = torch.tensor(list(attributes_info.values()), dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        return frame, keypoints, label, traffic_info, vehicle_info, appearance_info, attributes_info\n",
    "\n",
    "\n",
    "\n",
    "# Data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "frames_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\JAAD_clips\\frames_with_bboxes'\n",
    "keypoints_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\JAAD_clips\\pose_keypoints'\n",
    "cache_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\cache'\n",
    "\n",
    "base_dataset = JAADDataset(frames_dir, keypoints_dir, cache_dir, transform)\n",
    "\n",
    "all_labels = []\n",
    "for i in range(len(base_dataset)):\n",
    "    _, _, label, _, _, _, _ = base_dataset[i]\n",
    "    all_labels.append(int(label))\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "total_frames = len(all_labels)\n",
    "n_crossing = (all_labels == 1).sum()\n",
    "n_not_crossing = (all_labels == 0).sum()\n",
    "\n",
    "print(\"\\n STATYSTYKI ZBIORU \")\n",
    "print(f\"Łącznie klatek:      {total_frames}\")\n",
    "print(f\"Crossing (1):        {n_crossing}  ({n_crossing / total_frames:.2%})\")\n",
    "print(f\"Not crossing (0):    {n_not_crossing}  ({n_not_crossing / total_frames:.2%})\")\n",
    "\n",
    "\n",
    "train_indices, test_indices = train_test_split(list(range(len(base_dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_set = Subset(base_dataset, train_indices)\n",
    "test_set = Subset(base_dataset, test_indices)\n",
    "\n",
    "\n",
    "train_save_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\training_data'\n",
    "test_save_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\test_data'\n",
    "os.makedirs(train_save_dir, exist_ok=True)\n",
    "os.makedirs(test_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "if not os.listdir(train_save_dir) or not os.listdir(test_save_dir):\n",
    "\n",
    "    def save_preprocessed_data(dataset, save_dir):\n",
    "        for idx in tqdm(range(len(dataset)), desc=f\"Saving to {os.path.basename(save_dir)}\", unit=\"frame\"):\n",
    "            frame, keypoints, label, traffic_info, vehicle_info, appearance_info, attributes_info = dataset[idx]\n",
    "            save_path = os.path.join(save_dir, f'data_{idx}.pt')\n",
    "            torch.save({\n",
    "                'frame': frame,\n",
    "                'keypoints': keypoints,\n",
    "                'label': label,\n",
    "                'traffic_info': traffic_info,\n",
    "                'vehicle_info': vehicle_info,\n",
    "                'appearance_info': appearance_info,\n",
    "                'attributes_info': attributes_info\n",
    "            }, save_path)\n",
    "\n",
    "\n",
    "    save_preprocessed_data(train_set, train_save_dir)\n",
    "    save_preprocessed_data(test_set, test_save_dir)\n",
    "else:\n",
    "  print(\"Training data and testing data already exist. Skipping preprocessing.\")\n",
    "\n",
    "\n",
    "class PreprocessedDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pt')])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.data_files[idx])\n",
    "        frame = data['frame']\n",
    "        keypoints = data['keypoints']\n",
    "        label = data['label']\n",
    "        traffic_info = data['traffic_info']\n",
    "        vehicle_info = data['vehicle_info']\n",
    "        appearance_info = data['appearance_info']\n",
    "        attributes_info = data['attributes_info']\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        return frame, keypoints, label, traffic_info, vehicle_info, appearance_info, attributes_info\n",
    "\n",
    "train_dataset = PreprocessedDataset(train_save_dir, transform=None)\n",
    "test_dataset = PreprocessedDataset(test_save_dir, transform=None)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, keypoints, labels, traffic_infos, vehicle_infos, appearance_infos, attributes_infos = zip(*batch)\n",
    "\n",
    "    frames = torch.stack(frames)\n",
    "    keypoints = torch.stack(keypoints)\n",
    "    labels = torch.tensor(labels)\n",
    "    traffic_infos = torch.stack(traffic_infos)\n",
    "    vehicle_infos = torch.stack(vehicle_infos)\n",
    "    appearance_infos = torch.stack(appearance_infos)\n",
    "    attributes_infos = torch.stack(attributes_infos)\n",
    "\n",
    "    return frames, keypoints, labels, traffic_infos, vehicle_infos, appearance_infos, attributes_infos\n",
    "\n",
    "\n",
    "print(\"Datasets created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d_j4gPftM4M"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSUsxswhYh1l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PedestrianCrossingPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PedestrianCrossingPredictor, self).__init__()\n",
    "        \n",
    "        # VGG19 backbone\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        self.vgg19_features = vgg19.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.vgg19_classifier = nn.Sequential(*list(vgg19.classifier.children())[:-1])  # do 4096-D\n",
    "        \n",
    "        for param in self.vgg19_features[:36].parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # dimensionality rediction\n",
    "        self.fc_img = nn.Sequential(\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # goal module\n",
    "        self.goal_module = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # keypoints (99) + traffic (4) + vehicle (1) + appearance (3) + attributes (3) = 110\n",
    "        self.com_fc1 = nn.Linear(256 + 256 + 110, 256)\n",
    "        self.com_bn1 = nn.BatchNorm1d(256)\n",
    "        self.com_d1 = nn.Dropout(0.5)\n",
    "        self.com_fc2 = nn.Linear(256, 128)\n",
    "        self.com_bn2 = nn.BatchNorm1d(128)\n",
    "        self.com_d2 = nn.Dropout(0.5)\n",
    "        self.com_fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, keypoints, traffic_info, vehicle_info, appearance_info, attributes_info):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # features\n",
    "        c_out = self.vgg19_features(x)\n",
    "        c_out = self.avgpool(c_out)\n",
    "        c_out = torch.flatten(c_out, 1)\n",
    "        c_out = self.vgg19_classifier(c_out)  # [batch, 4096]\n",
    "        c_out = self.fc_img(c_out)            # [batch, 256]\n",
    "        \n",
    "        # additional data\n",
    "        keypoints = keypoints.view(batch_size, -1)  # 33x3=99\n",
    "        additional_info = torch.cat([keypoints, traffic_info, vehicle_info, appearance_info, attributes_info], dim=1)\n",
    "        \n",
    "        # goal module\n",
    "        goal_out = self.goal_module(c_out)\n",
    "        \n",
    "        combined = torch.cat((c_out, goal_out, additional_info), dim=1)\n",
    "        combined = self.com_fc1(combined)\n",
    "        combined = torch.relu(combined)\n",
    "        combined = self.com_bn1(combined)\n",
    "        combined = self.com_d1(combined)\n",
    "        combined = self.com_fc2(combined)\n",
    "        combined = torch.relu(combined)\n",
    "        combined = self.com_bn2(combined)\n",
    "        combined = self.com_d2(combined)\n",
    "        combined = self.com_fc3(combined)\n",
    "        \n",
    "        return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5xFMpzWYUnG",
    "outputId": "589a9f33-621e-4e61-b39e-25cc896d801e"
   },
   "outputs": [],
   "source": [
    "# weights initialization\n",
    "def init_w(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = PedestrianCrossingPredictor()\n",
    "model.apply(init_w)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "torch.save(model.state_dict(), r'D:\\mgr\\PedestrianIntentionEstimation\\model\\modelrn.pth')\n",
    "torch.save(optimizer.state_dict(), r'D:\\mgr\\PedestrianIntentionEstimation\\model\\optimizerrn.pth')\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSHLQLFltWDB"
   },
   "source": [
    "# Training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOGr2OKEdXVZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history['loss'], marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history['accuracy'], marker='o')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Recall\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history['recall'], marker='o')\n",
    "    plt.title('Training Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # F1 Score\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, history['f1'], marker='o')\n",
    "    plt.title('Training F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from multiprocessing import cpu_count\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessedDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pt')])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.data_files[idx])\n",
    "        frame = data['frame']\n",
    "        keypoints = data['keypoints']\n",
    "        label = data['label']\n",
    "        traffic_info = data['traffic_info']\n",
    "        vehicle_info = data['vehicle_info']\n",
    "        appearance_info = data['appearance_info']\n",
    "        attributes_info = data['attributes_info']\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        return frame, keypoints, label, traffic_info, vehicle_info, appearance_info, attributes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(y_true, y_probs):\n",
    "    ths = np.linspace(0.0, 1.0, 101)\n",
    "    accs = [accuracy_score(y_true, (y_probs >= t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(accs))\n",
    "    return float(ths[i]), float(accs[i])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\training_data'\n",
    "train_pt = PreprocessedDataset(train_dir, transform=None)\n",
    "\n",
    "labels = []\n",
    "for i in tqdm(range(len(train_pt)), desc=\"Reading labels\"):\n",
    "    item = train_pt[i]\n",
    "    labels.append(int(item[2]))  # [2] = label\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "# save labels to npy file\n",
    "out_path = r'D:\\mgr\\PedestrianIntentionEstimation\\train_labels_all.npy'\n",
    "np.save(out_path, labels)\n",
    "print(f\"Saved {labels.shape[0]} labels to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c60do8dotcXD"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from multiprocessing import cpu_count\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    ths = np.linspace(0.0, 1.0, 101)\n",
    "    accs = [accuracy_score(y_true, (y_probs >= t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(accs))\n",
    "    return float(ths[i]), float(accs[i])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dir = r'D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\training_data'\n",
    "train_pt = PreprocessedDataset(train_dir, transform=None)\n",
    "\n",
    "# Labels\n",
    "LABELS_FILE = r'D:\\mgr\\PedestrianIntentionEstimation\\train_labels_all.npy'\n",
    "if os.path.exists(LABELS_FILE):\n",
    "    labels = np.load(LABELS_FILE)\n",
    "    print(f\"[labels] Loaded from {LABELS_FILE} -> {labels.shape[0]} samples\")\n",
    "else:\n",
    "    print(\"[labels] Building from dataset...\")\n",
    "    labels = []\n",
    "    for i in tqdm(range(len(train_pt)), desc=\"Reading labels\"):\n",
    "        _, _, lab, _, _, _, _ = train_pt[i]\n",
    "        labels.append(int(lab))\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    np.save(LABELS_FILE, labels)\n",
    "    print(f\"[labels] Saved to {LABELS_FILE}\")\n",
    "\n",
    "# Model\n",
    "model = PedestrianCrossingPredictor().to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    r'D:\\mgr\\PedestrianIntentionEstimation\\model\\modelrn.pth',\n",
    "    map_location=device\n",
    "))\n",
    "\n",
    "# Balancing classes\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "crossing_idxs = np.where(labels == 1)[0]\n",
    "non_crossing_idxs = np.where(labels == 0)[0]\n",
    "\n",
    "n_keep = min(len(crossing_idxs), len(non_crossing_idxs))\n",
    "crossing_keep = rng.choice(crossing_idxs, size=n_keep, replace=False)\n",
    "non_crossing_keep = rng.choice(non_crossing_idxs, size=n_keep, replace=False)\n",
    "\n",
    "final_idxs = np.concatenate([crossing_keep, non_crossing_keep])\n",
    "rng.shuffle(final_idxs)\n",
    "\n",
    "print(f\"Crossing samples kept:      {len(crossing_keep)}\")\n",
    "print(f\"Not-crossing samples kept:  {len(non_crossing_keep)}\")\n",
    "print(f\"Total available (balanced): {len(final_idxs)}\")\n",
    "\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    final_idxs, test_size=0.10, random_state=42, shuffle=True, stratify=labels[final_idxs]\n",
    ")\n",
    "train_subset = Subset(train_pt, train_idx)\n",
    "val_subset   = Subset(train_pt, val_idx)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, keypoints, labels_b, traffic, vehicle, appearance, attrs = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    keypoints = torch.stack(keypoints)\n",
    "    labels_b = torch.tensor(labels_b)\n",
    "    traffic = torch.stack(traffic)\n",
    "    vehicle = torch.stack(vehicle)\n",
    "    appearance = torch.stack(appearance)\n",
    "    attrs = torch.stack(attrs)\n",
    "    return frames, keypoints, labels_b, traffic, vehicle, appearance, attrs\n",
    "\n",
    "n_w = min(16, cpu_count())\n",
    "comb_train_loader = DataLoader(\n",
    "    train_subset, batch_size=32, shuffle=True,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_subset, batch_size=64, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# evaluation and train\n",
    "def evaluate_on_loader(model, loader, criterion, device, threshold=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_probs, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for frames, keypoints, labels_batch, traffic, vehicle, appearance, attrs in loader:\n",
    "            frames = frames.to(device); keypoints = keypoints.to(device)\n",
    "            labels_batch = labels_batch.to(device); traffic = traffic.to(device)\n",
    "            vehicle = vehicle.to(device); appearance = appearance.to(device); attrs = attrs.to(device)\n",
    "            with autocast():\n",
    "                out = model(frames, keypoints, traffic, vehicle, appearance, attrs)\n",
    "                loss = criterion(out, labels_batch.float().view(-1,1))\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "            all_probs.extend(torch.sigmoid(out).cpu().numpy().flatten())\n",
    "            all_targets.extend(labels_batch.cpu().numpy().flatten())\n",
    "    avg_loss = running_loss / len(loader.dataset)\n",
    "    all_probs = np.array(all_probs); all_targets = np.array(all_targets).astype(int)\n",
    "    thr = find_best_threshold(all_targets, all_probs)[0] if threshold is None else threshold\n",
    "    preds = (all_probs >= thr).astype(int)\n",
    "    acc = accuracy_score(all_targets, preds)\n",
    "    rec = recall_score(all_targets, preds)\n",
    "    f1  = f1_score(all_targets, preds)\n",
    "    return avg_loss, acc, rec, f1, thr, all_targets, preds\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, scheduler=None,\n",
    "          num_epochs=10, device='cuda', verbose=True,\n",
    "          patience=10, min_best_epoch=5, min_delta=1e-4,\n",
    "          model_save_path=r'D:\\mgr\\PedestrianIntentionEstimation\\model\\trained_modelrnall30.pth'):\n",
    "\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    history = {\n",
    "        'loss': [], 'accuracy': [], 'recall': [], 'f1': [], 'threshold': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_recall': [], 'val_f1': [], 'val_threshold': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, probs_all, targets_all = 0.0, [], []\n",
    "\n",
    "        for bidx, (frames, keypoints, labels_b, traffic, vehicle, appearance, attrs) in \\\n",
    "                enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")):\n",
    "\n",
    "            if epoch == 0 and bidx == 0:\n",
    "                print(\"Labels in first batch:\", labels_b.cpu().numpy())\n",
    "                print(\"Number of crossing (1):\", (labels_b == 1).sum().item())\n",
    "                print(\"Number of not crossing (0):\", (labels_b == 0).sum().item())\n",
    "\n",
    "            frames = frames.to(device); keypoints = keypoints.to(device)\n",
    "            labels_b = labels_b.to(device); traffic = traffic.to(device)\n",
    "            vehicle = vehicle.to(device); appearance = appearance.to(device); attrs = attrs.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                out = model(frames, keypoints, traffic, vehicle, appearance, attrs)\n",
    "                loss = criterion(out, labels_b.float().view(-1, 1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * frames.size(0)\n",
    "            probs_all.extend(torch.sigmoid(out).detach().cpu().numpy().flatten())\n",
    "            targets_all.extend(labels_b.detach().cpu().numpy().flatten())\n",
    "\n",
    "        # train metrics\n",
    "        train_preds = (np.array(probs_all) >= 0.5).astype(int)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc  = accuracy_score(targets_all, train_preds)\n",
    "        train_rec  = recall_score(targets_all, train_preds)\n",
    "        train_f1   = f1_score(targets_all, train_preds)\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc, val_rec, val_f1, val_thr, _, _ = evaluate_on_loader(\n",
    "            model, val_loader, criterion, device, threshold=None\n",
    "        )\n",
    "\n",
    "        history['loss'].append(train_loss); history['accuracy'].append(train_acc)\n",
    "        history['recall'].append(train_rec); history['f1'].append(train_f1); history['threshold'].append(0.5)\n",
    "        history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "        history['val_recall'].append(val_rec); history['val_f1'].append(val_f1); history['val_threshold'].append(val_thr)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                  f\"Train: L {train_loss:.4f} A {train_acc:.4f} R {train_rec:.4f} F1 {train_f1:.4f} | \"\n",
    "                  f\"Val:   L {val_loss:.4f} A {val_acc:.4f} R {val_rec:.4f} F1 {val_f1:.4f} Thr* {val_thr:.2f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if (epoch + 1) > min_best_epoch:\n",
    "            if val_loss < best_val_loss - min_delta:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping: no val_loss improvement ≥ {min_delta} for {patience} epochs after epoch {min_best_epoch}.\")\n",
    "                    break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Train\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "trained_m, history = train(\n",
    "    model, comb_train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "    num_epochs=30, device=device, verbose=True, patience=5, min_best_epoch=5\n",
    ")\n",
    "\n",
    "plot_metrics({'loss': history['loss'], 'accuracy': history['accuracy'],\n",
    "              'recall': history['recall'], 'f1': history['f1']})\n",
    "\n",
    "# Plots\n",
    "epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, history['loss'], marker='o', label='Train Loss')\n",
    "plt.plot(epochs, history['val_loss'], marker='o', label='Val Loss')\n",
    "plt.title('Loss (train vs val)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.grid(True); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, history['accuracy'], marker='o', label='Train Acc')\n",
    "plt.plot(epochs, history['val_acc'], marker='o', label='Val Acc')\n",
    "plt.title('Accuracy (train vs val)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.grid(True); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.cuda.amp import autocast\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "TEST_DIR = r\"D:\\mgr\\PedestrianIntentionEstimation\\JAAD_dataset\\test_data\"\n",
    "LABELS_FILE = r\"D:\\mgr\\PedestrianIntentionEstimation\\test_labelsrn.npy\"\n",
    "MODEL_CKPT = r\"D:\\mgr\\PedestrianIntentionEstimation\\model\\trained_modelrnall30.pth\"\n",
    "\n",
    "SAMPLES_PER_CLASS = 5000 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PreprocessedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.pt')])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.data_files[idx], map_location=\"cpu\")\n",
    "        frame = data['frame']               # Tensor [3,224,224]\n",
    "        keypoints = data['keypoints']       # Tensor [33,3]\n",
    "        label = data['label']               # int 0/1\n",
    "        traffic_info = data['traffic_info'] # Tensor [4]\n",
    "        vehicle_info = data['vehicle_info'] # Tensor [1]\n",
    "        appearance_info = data['appearance_info'] # Tensor [3]\n",
    "        attributes_info = data['attributes_info'] # Tensor [3]\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "        return frame, keypoints, label, traffic_info, vehicle_info, appearance_info, attributes_info\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, keypoints, labels_b, traffic, vehicle, appearance, attrs = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    keypoints = torch.stack(keypoints)\n",
    "    labels_b = torch.tensor(labels_b)\n",
    "    traffic = torch.stack(traffic)\n",
    "    vehicle = torch.stack(vehicle)\n",
    "    appearance = torch.stack(appearance)\n",
    "    attrs = torch.stack(attrs)\n",
    "    return frames, keypoints, labels_b, traffic, vehicle, appearance, attrs\n",
    "\n",
    "test_pt = PreprocessedDataset(TEST_DIR, transform=None)\n",
    "\n",
    "# Labels\n",
    "if os.path.exists(LABELS_FILE):\n",
    "    labels_all = np.load(LABELS_FILE)\n",
    "    print(f\"[labels] Loaded from {LABELS_FILE} -> {labels_all.shape[0]} samples\")\n",
    "else:\n",
    "    print(\"[labels] Building from dataset...\")\n",
    "    labels_all = []\n",
    "    for i in tqdm(range(len(test_pt)), desc=\"Extracting labels\"):\n",
    "        _, _, lab, _, _, _, _ = test_pt[i]\n",
    "        labels_all.append(int(lab))\n",
    "    labels_all = np.array(labels_all, dtype=np.int8)\n",
    "    np.save(LABELS_FILE, labels_all)\n",
    "    print(f\"[labels] Saved to {LABELS_FILE}\")\n",
    "\n",
    "# Test set\n",
    "total_samples = labels_all.shape[0]\n",
    "n_crossing = int((labels_all == 1).sum())\n",
    "n_not_crossing = total_samples - n_crossing\n",
    "print(\"\\n STATYSTYKI ZBIORU TESTOWEGO \")\n",
    "print(f\"Łącznie próbek:      {total_samples}\")\n",
    "print(f\"Crossing (1):        {n_crossing}  ({n_crossing / max(1,total_samples):.2%})\")\n",
    "print(f\"Not crossing (0):    {n_not_crossing}  ({n_not_crossing / max(1,total_samples):.2%})\")\n",
    "\n",
    "# Model\n",
    "class PedestrianCrossingPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        import torchvision.models as models\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        self.vgg19_features = vgg19.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.vgg19_classifier = nn.Sequential(*list(vgg19.classifier.children())[:-1])  # 4096-D\n",
    "\n",
    "        for p in self.vgg19_features[:36].parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc_img = nn.Sequential(\n",
    "            nn.Linear(4096, 512), nn.ReLU(), nn.BatchNorm1d(512), nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256), nn.ReLU()\n",
    "        )\n",
    "        self.goal_module = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256), nn.ReLU()\n",
    "        )\n",
    "        # 99 keypoints + (4+1+3+3)=11 context = 110\n",
    "        self.com_fc1 = nn.Linear(256 + 256 + 110, 256)\n",
    "        self.com_bn1 = nn.BatchNorm1d(256)\n",
    "        self.com_d1 = nn.Dropout(0.5)\n",
    "        self.com_fc2 = nn.Linear(256, 128)\n",
    "        self.com_bn2 = nn.BatchNorm1d(128)\n",
    "        self.com_d2 = nn.Dropout(0.5)\n",
    "        self.com_fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, keypoints, traffic_info, vehicle_info, appearance_info, attributes_info):\n",
    "        # x: [B,3,224,224]\n",
    "        c_out = self.vgg19_features(x)\n",
    "        c_out = self.avgpool(c_out)\n",
    "        c_out = torch.flatten(c_out, 1)\n",
    "        c_out = self.vgg19_classifier(c_out)  # [B,4096]\n",
    "        c_out = self.fc_img(c_out)            # [B,256]\n",
    "\n",
    "        B = x.size(0)\n",
    "        keypoints = keypoints.view(B, -1)     # [B,99]\n",
    "        additional_info = torch.cat([keypoints, traffic_info, vehicle_info, appearance_info, attributes_info], dim=1)  # [B,110]\n",
    "\n",
    "        goal_out = self.goal_module(c_out)\n",
    "        combined = torch.cat((c_out, goal_out, additional_info), dim=1)\n",
    "        combined = torch.relu(self.com_fc1(combined))\n",
    "        combined = self.com_bn1(combined)\n",
    "        combined = self.com_d1(combined)\n",
    "        combined = torch.relu(self.com_fc2(combined))\n",
    "        combined = self.com_bn2(combined)\n",
    "        combined = self.com_d2(combined)\n",
    "        logits = self.com_fc3(combined)       # [B,1]\n",
    "        return logits\n",
    "\n",
    "model = PedestrianCrossingPredictor().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_CKPT, map_location=device))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Test sampling\n",
    "crossing_idx = np.where(labels_all == 1)[0].tolist()\n",
    "non_crossing_idx = np.where(labels_all == 0)[0].tolist()\n",
    "crossing_sampled = random.sample(crossing_idx, min(SAMPLES_PER_CLASS, len(crossing_idx)))\n",
    "non_crossing_sampled = random.sample(non_crossing_idx, min(SAMPLES_PER_CLASS, len(non_crossing_idx)))\n",
    "balanced_indices = crossing_sampled + non_crossing_sampled\n",
    "random.shuffle(balanced_indices)\n",
    "\n",
    "balanced_test = Subset(test_pt, balanced_indices)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    frames, keypoints, labels_b, traffic, vehicle, appearance, attrs = zip(*batch)\n",
    "    frames = torch.stack(frames)\n",
    "    keypoints = torch.stack(keypoints)\n",
    "    labels_b = torch.tensor(labels_b)\n",
    "    traffic = torch.stack(traffic)\n",
    "    vehicle = torch.stack(vehicle)\n",
    "    appearance = torch.stack(appearance)\n",
    "    attrs = torch.stack(attrs)\n",
    "    return frames, keypoints, labels_b, traffic, vehicle, appearance, attrs\n",
    "\n",
    "n_w = min(16, cpu_count())\n",
    "balanced_loader = DataLoader(\n",
    "    balanced_test, batch_size=32, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    ths = np.arange(0.0, 1.01, 0.01)\n",
    "    best_t, best_acc = 0.5, 0.0\n",
    "    for t in ths:\n",
    "        preds = (y_probs >= t).astype(int)\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc, best_t = acc, t\n",
    "    return best_t, best_acc\n",
    "\n",
    "def test(model, criterion, test_loader, ablation=None):\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    all_probs, test_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frames, keypoints, labels, traffic_info, vehicle_info, appearance_info, attributes_info in tqdm(\n",
    "            test_loader, desc=\"Testing\", unit=\"batch\"\n",
    "        ):\n",
    "            frames = frames.to(device); keypoints = keypoints.to(device)\n",
    "            traffic_info = traffic_info.to(device); vehicle_info = vehicle_info.to(device)\n",
    "            appearance_info = appearance_info.to(device); attributes_info = attributes_info.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if ablation == 'traffic':\n",
    "                traffic_info = torch.zeros_like(traffic_info)\n",
    "            elif ablation == 'vehicle':\n",
    "                vehicle_info = torch.zeros_like(vehicle_info)\n",
    "            elif ablation == 'appearance':\n",
    "                appearance_info = torch.zeros_like(appearance_info)\n",
    "            elif ablation == 'attributes':\n",
    "                attributes_info = torch.zeros_like(attributes_info)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(frames, keypoints, traffic_info, vehicle_info, appearance_info, attributes_info)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "            test_running_loss += loss.item() * frames.size(0)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().reshape(-1)\n",
    "            all_probs.extend(probs)\n",
    "            test_targets.extend(labels.cpu().numpy().reshape(-1))\n",
    "\n",
    "    avg_test_loss = test_running_loss / len(test_loader.dataset)\n",
    "    all_probs = np.array(all_probs)\n",
    "    test_targets = np.array(test_targets).astype(int)\n",
    "\n",
    "    best_thresh, best_acc = find_best_threshold(test_targets, all_probs)\n",
    "    preds = (all_probs >= best_thresh).astype(int)\n",
    "    test_recall = recall_score(test_targets, preds)\n",
    "    test_f1 = f1_score(test_targets, preds)\n",
    "    return avg_test_loss, best_acc, test_recall, test_f1, best_thresh, test_targets, preds\n",
    "\n",
    "# Test\n",
    "avg_loss, acc, rec, f1, thr, targets, preds = test(model, criterion, balanced_loader)\n",
    "\n",
    "print(f\"\\nWyniki testu (po {SAMPLES_PER_CLASS} na klasę) \")\n",
    "print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "print(f\"Best Threshold: {thr:.2f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(targets, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Crossing\", \"Crossing\"])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f\"Confusion Matrix (Thr={thr:.2f}, {SAMPLES_PER_CLASS} per class)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
